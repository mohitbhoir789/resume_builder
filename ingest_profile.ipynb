{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6bf876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q fastapi uvicorn pydantic pydantic-settings python-dotenv pdfplumber requests httpx python-multipart jinja2 loguru google-genai sentence-transformers torch numpy scikit-learn faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e574cd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "> **Important**: Set your Gemini API key before running:\n",
    "> ```bash\n",
    "> export GOOGLE_API_KEY=\"your-api-key-here\"\n",
    "> ```\n",
    "> Get your key from https://makersuite.google.com/app/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66356e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Profile cache directory: /Users/mohitbhoir/Git/resume_builder/profile_cache\n",
      "‚úÖ Ready to ingest resume\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, str(Path.cwd() / 'backend'))\n",
    "\n",
    "from app.services.ingest import IngestService\n",
    "from app.models.schemas import ProfileInput\n",
    "\n",
    "# Create output directory for stored profiles\n",
    "PROFILE_CACHE_DIR = Path.cwd() / 'profile_cache'\n",
    "PROFILE_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Profile cache directory: {PROFILE_CACHE_DIR}\")\n",
    "print(f\"‚úÖ Ready to ingest resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34827f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Gemini API key (get it from https://makersuite.google.com/app/apikey)\n",
    "import os\n",
    "\n",
    "GEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual API key\n",
    "\n",
    "if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\":\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "    print(\"‚úÖ Gemini API key configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Gemini API key not set. Using regex fallback for section extraction.\")\n",
    "    print(\"   To enable AI-powered extraction:\")\n",
    "    print(\"   1. Get your key from: https://makersuite.google.com/app/apikey\")\n",
    "    print(\"   2. Replace GEMINI_API_KEY above with your actual key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27959cc8",
   "metadata": {},
   "source": [
    "## Option 1: Ingest from PDF File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e564627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your PDF file path\n",
    "PDF_FILE_PATH = \"/Users/mohitbhoir/Git/resume_builder/resume-2.pdf\"\n",
    "\n",
    "if Path(PDF_FILE_PATH).exists():\n",
    "    from fastapi import UploadFile\n",
    "    from io import BytesIO\n",
    "    \n",
    "    # Create a mock UploadFile\n",
    "    class MockUploadFile:\n",
    "        def __init__(self, file_path):\n",
    "            self.file_path = Path(file_path)\n",
    "            self.content_type = \"application/pdf\"\n",
    "            self.file = BytesIO(self.file_path.read_bytes())\n",
    "    \n",
    "    service = IngestService()\n",
    "    mock_file = MockUploadFile(PDF_FILE_PATH)\n",
    "    result = service.ingest_pdf(mock_file)\n",
    "    \n",
    "    print(f\"‚úÖ PDF ingested successfully\")\n",
    "    print(f\"   Sections: {result.sections}\")\n",
    "    print(f\"   Chunks: {result.chunks_created}\")\n",
    "    print(f\"   Embedding provider: {result.embedding_provider}\")\n",
    "    print(f\"\\nProfile data:\")\n",
    "    print(f\"   Experience items: {len(result.profile.experience)}\")\n",
    "    print(f\"   Projects: {len(result.profile.projects)}\")\n",
    "    print(f\"   Skills: {len(result.profile.skills)}\")\n",
    "    print(f\"   Education: {len(result.profile.education)}\")\n",
    "else:\n",
    "    print(f\"‚ùå PDF file not found at {PDF_FILE_PATH}\")\n",
    "    print(f\"   Update PDF_FILE_PATH above with your resume path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed70192",
   "metadata": {},
   "source": [
    "## Option 2: Ingest from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Replace with your resume PDF URL\n",
    "PDF_URL = \"https://example.com/path/to/resume.pdf\"\n",
    "\n",
    "try:\n",
    "    # Download PDF from URL\n",
    "    response = requests.get(PDF_URL, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Create mock upload file\n",
    "    class MockUploadFile:\n",
    "        def __init__(self, content):\n",
    "            self.content_type = \"application/pdf\"\n",
    "            self.file = BytesIO(content)\n",
    "    \n",
    "    service = IngestService()\n",
    "    mock_file = MockUploadFile(response.content)\n",
    "    result = service.ingest_pdf(mock_file)\n",
    "    \n",
    "    print(f\"‚úÖ PDF from URL ingested successfully\")\n",
    "    print(f\"   Sections: {result.sections}\")\n",
    "    print(f\"   Chunks: {result.chunks_created}\")\n",
    "    print(f\"\\nProfile data:\")\n",
    "    print(f\"   Experience items: {len(result.profile.experience)}\")\n",
    "    print(f\"   Projects: {len(result.profile.projects)}\")\n",
    "    print(f\"   Skills: {len(result.profile.skills)}\")\n",
    "    print(f\"   Education: {len(result.profile.education)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading PDF: {e}\")\n",
    "    print(f\"   Update PDF_URL above with your resume URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30eb582",
   "metadata": {},
   "source": [
    "## Option 3: Ingest from Plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00d246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text ingested successfully\n",
      "   Sections: ['experience', 'projects', 'skills', 'education', 'certifications']\n",
      "   Chunks: 0\n",
      "\n",
      "Profile data:\n",
      "   Experience items: 0\n",
      "   Projects: 0\n",
      "   Skills: 0\n",
      "   Education: 0\n"
     ]
    }
   ],
   "source": [
    "# Paste your resume text here\n",
    "RESUME_TEXT = \"\"\"\n",
    "Mohit Bhoir\n",
    "Willimantic, CT 06226 | 959-995-0104 | mohitbhoir789@gmail.com\n",
    "LinkedIn: [linkedin.com/in/mohitbhoir789](https://linkedin.com/in/mohitbhoir789)\n",
    "GitHub: [github.com/mohitbhoir789](https://github.com/mohitbhoir789)\n",
    "Portfolio: mohitbhoir789.github.io/portfolio/\n",
    "\n",
    "Technical Skills\n",
    "\n",
    "* Programming Languages: Python, Java, JavaScript, C++, R, SQL\n",
    "* Data Science & ML Frameworks: TensorFlow, PyTorch, Scikit-Learn, Keras, Pandas, NumPy, Matplotlib, SciPy; Machine Learning (Classification, Regression, Clustering, NLP, Time-Series, Data Mining, Statistics)\n",
    "* Databases & MLOps: PostgreSQL, MySQL, MongoDB; Docker, Airflow, MLflow, CI/CD Pipelines\n",
    "* Cloud Platforms & Tools: AWS, GitHub, Jupyter, Power BI, Tableau, Hadoop, ETL Tools, PyCharm, MCP, AI Agents\n",
    "\n",
    "Work Experience\n",
    "\n",
    "Community Dreams Foundation | Data Science Intern | Sep 2025 - Dec 2025\n",
    "\n",
    "* Worked on end-to-end data science applications involving data collection, preprocessing, modeling, and deployment for social impact initiatives.\n",
    "* Processed and analyzed large-scale structured and unstructured datasets (1M+ records) using Python, Pandas, NumPy, and SQL to derive actionable insights.\n",
    "* Built and evaluated machine learning models including Logistic Regression, Random Forest, XGBoost, and Gradient Boosting for prediction and classification tasks, improving model accuracy by up to 22%.\n",
    "* Developed data pipelines for cleaning, feature engineering, and transformation, reducing data inconsistencies by 30%.\n",
    "* Applied statistical analysis and hypothesis testing to identify trends, correlations, and key performance drivers across datasets.\n",
    "* Implemented model evaluation techniques such as cross-validation, ROC-AUC, precision-recall, and confusion matrices to ensure robustness and reliability.\n",
    "* Collaborated with cross-functional teams to translate business requirements into scalable data science solutions and dashboards.\n",
    "* Deployed trained models using Python-based workflows and versioned experiments using MLflow and Git for reproducibility.\n",
    "\n",
    "Beats by Dre | Data Science & Consumer Insights Extern | Jun 2025 - Aug 2025\n",
    "\n",
    "* Performed exploratory data analysis and sentiment analysis on over 5,000 Amazon reviews using Python, BeautifulSoup, TextBlob, Pandas, and Seaborn to uncover product sentiment, preferences, and brand positioning.\n",
    "* Segmented customer survey data using Pandas and NumPy to define user personas based on generation, price sensitivity, and feature prioritization.\n",
    "* Extracted key demand drivers such as bass-forward sound, battery life, and design aesthetics through polarity scores, frequency distributions, and word cloud visualizations.\n",
    "* Automated the data scraping pipeline using BeautifulSoup and OxyLabs, reducing manual data collection time by 90% /and improving dataset scale and quality.\n",
    "* Translated technical findings into data-backed launch recommendations, including product specifications, pricing, and go-to-market strategy.\n",
    "* Applied skills in data scraping, text preprocessing, EDA, sentiment analysis, and survey segmentation to support product strategy and consumer insight generation.\n",
    "\n",
    "Amdocs, India | Software Development Engineer | Jul 2021 - Jul 2024\n",
    "\n",
    "* Automated regression test suites using Selenium and Robot Framework, reducing release cycles by 30% /and boosting deployment efficiency.\n",
    "* Engineered scalable ServiceNow workflows that eliminated 40% /of repetitive manual tasks, streamlining request processing time by 25%.\n",
    "* Built real-time dashboards in Power BI, enhancing reporting efficiency and enabling data-driven decision-making for key stakeholders.\n",
    "* Collaborated with 4+ cross-functional teams to design AI-based automation tools, improving accuracy and team productivity by 20%.\n",
    "* Integrated regression testing pipelines within development cycles, achieving 95% test accuracy and accelerating QA feedback loops.\n",
    "* Developed scripts to test APIs for Order Management System (OMS) and Customer Service Provisioning (CSP), ensuring robust backend integration and improving reliability.\n",
    "\n",
    "Projects\n",
    "\n",
    "Movie Recommendation Chatbot | Python, RAG, Hugging Face, Pinecone, PostgreSQL, TMDb API\n",
    "\n",
    "* Developed a semantic-search-based chatbot using a dataset of 343K+ IMDb movies (2000-2024).\n",
    "* Used Hugging Face embeddings + Pinecone vector DB to enable real-time recommendation retrieval with under 1-second latency.\n",
    "* Managed metadata for 200K+ unique movie entries via PostgreSQL; enriched data using TMDb API to improve content coverage by 30%.\n",
    "* Achieved over 92% /accuracy in matching user query intents to relevant movie descriptions through embedding tuning.\n",
    "\n",
    "Statistical Analysis of Corporate Takeovers | Python, Scikit-learn, Regression, XGBoost\n",
    "\n",
    "* Analyzed takeover data from 126 U.S. firms over 8 years to identify predictors of acquisition likelihood.\n",
    "* Built classification models (Poisson regression, Random Forest, XGBoost); logistic regression model achieved AUC = 0.78 and 77% /accuracy.\n",
    "* Applied scaling and feature selection to reduce model variance by 25% /and improve interpretability.\n",
    "\n",
    "Cricket Analysis Dashboard | SQL, Tableau, LOD, KPI Metrics\n",
    "\n",
    "* Created an interactive Tableau dashboard using 1.2M+ ODI ball-by-ball records (2002-2023) to analyze player and team performance.\n",
    "* Performed SQL-based ETL and implemented LOD calculations to generate 20+ KPIs for match summaries, venue stats, and team trends.\n",
    "* Improved data cleanliness and consistency by 35% through custom data wrangling scripts.\n",
    "\n",
    "Education\n",
    "\n",
    "University of Connecticut | Master of Science in Data Science | Aug 2024 - Present\n",
    "\n",
    "* GPA: 3.79/4.0\n",
    "* Relevant Coursework: Statistics, Machine Learning, NLP, Deep Learning, Algorithms, Data Mining, Gen-AI.\n",
    "\n",
    "University of Mumbai | Bachelor of Engineering in Electronics Engineering | Aug 2017 - Jul 2021\n",
    "\n",
    "* GPA: 7.78/10\n",
    "* Relevant Coursework: Operating Systems, Python, Cryptography & System Security, DBMS\n",
    "\"\"\"\n",
    "\n",
    "if RESUME_TEXT.strip():\n",
    "    service = IngestService()\n",
    "    result = service.ingest_text(RESUME_TEXT)\n",
    "    \n",
    "    print(f\"‚úÖ Text ingested successfully\")\n",
    "    print(f\"   Sections: {result.sections}\")\n",
    "    print(f\"   Chunks: {result.chunks_created}\")\n",
    "    print(f\"\\nProfile data:\")\n",
    "    print(f\"   Experience items: {len(result.profile.experience)}\")\n",
    "    print(f\"   Projects: {len(result.profile.projects)}\")\n",
    "    print(f\"   Skills: {len(result.profile.skills)}\")\n",
    "    print(f\"   Education: {len(result.profile.education)}\")\n",
    "else:\n",
    "    print(\"‚ùå No resume text provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd872cc",
   "metadata": {},
   "source": [
    "## Save Profile and Embeddings Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01859209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Save the profile to a JSON file\n",
    "profile_name = \"my_profile\"  # Change this to a unique name if you have multiple profiles\n",
    "profile_file = PROFILE_CACHE_DIR / f\"{profile_name}_profile.json\"\n",
    "embeddings_file = PROFILE_CACHE_DIR / f\"{profile_name}_embeddings.pkl\"\n",
    "\n",
    "# Save profile\n",
    "with open(profile_file, 'w') as f:\n",
    "    json.dump(result.profile.dict(), f, indent=2)\n",
    "print(f\"‚úÖ Profile saved to: {profile_file}\")\n",
    "\n",
    "# Save embeddings vector store\n",
    "vector_store = service.vector_store\n",
    "with open(embeddings_file, 'wb') as f:\n",
    "    pickle.dump(vector_store, f)\n",
    "print(f\"‚úÖ Embeddings saved to: {embeddings_file}\")\n",
    "\n",
    "# Create a metadata file\n",
    "metadata = {\n",
    "    \"profile_name\": profile_name,\n",
    "    \"ingest_type\": result.ingest_type,\n",
    "    \"sections\": result.sections,\n",
    "    \"chunks_created\": result.chunks_created,\n",
    "    \"embedding_provider\": result.embedding_provider,\n",
    "}\n",
    "metadata_file = PROFILE_CACHE_DIR / f\"{profile_name}_metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úÖ Metadata saved to: {metadata_file}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All profile data saved successfully!\")\n",
    "print(f\"\\nüìù Profile name: {profile_name}\")\n",
    "print(f\"   Use this name in your resume generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9e3b8",
   "metadata": {},
   "source": [
    "## View Saved Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0715f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìÅ Available saved profiles:\\n\")\n",
    "\n",
    "metadata_files = list(PROFILE_CACHE_DIR.glob(\"*_metadata.json\"))\n",
    "\n",
    "if not metadata_files:\n",
    "    print(\"   No saved profiles found\")\n",
    "else:\n",
    "    for metadata_file in sorted(metadata_files):\n",
    "        with open(metadata_file) as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"üìÑ {metadata['profile_name']}\")\n",
    "        print(f\"   Type: {metadata['ingest_type']}\")\n",
    "        print(f\"   Sections: {', '.join(metadata['sections'])}\")\n",
    "        print(f\"   Chunks: {metadata['chunks_created']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73c3f2",
   "metadata": {},
   "source": [
    "## Load Saved Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de64f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify which profile to load\n",
    "profile_to_load = \"my_profile\"  # Change to your profile name\n",
    "\n",
    "profile_file = PROFILE_CACHE_DIR / f\"{profile_to_load}_profile.json\"\n",
    "\n",
    "if profile_file.exists():\n",
    "    with open(profile_file) as f:\n",
    "        loaded_profile = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded profile: {profile_to_load}\")\n",
    "    print(f\"\\nExperience ({len(loaded_profile['experience'])} items):\")\n",
    "    for i, exp in enumerate(loaded_profile['experience'][:3], 1):\n",
    "        print(f\"  {i}. {exp[:80]}...\" if len(exp) > 80 else f\"  {i}. {exp}\")\n",
    "    \n",
    "    print(f\"\\nProjects ({len(loaded_profile['projects'])} items):\")\n",
    "    for i, proj in enumerate(loaded_profile['projects'][:3], 1):\n",
    "        print(f\"  {i}. {proj[:80]}...\" if len(proj) > 80 else f\"  {i}. {proj}\")\n",
    "    \n",
    "    print(f\"\\nSkills ({len(loaded_profile['skills'])} items):\")\n",
    "    for i, skill in enumerate(loaded_profile['skills'][:5], 1):\n",
    "        print(f\"  {i}. {skill}\")\n",
    "    \n",
    "    print(f\"\\nEducation ({len(loaded_profile['education'])} items):\")\n",
    "    for i, edu in enumerate(loaded_profile['education'], 1):\n",
    "        print(f\"  {i}. {edu}\")\n",
    "else:\n",
    "    print(f\"‚ùå Profile not found: {profile_to_load}\")\n",
    "    print(f\"\\n   Available profiles:\")\n",
    "    for metadata_file in sorted(PROFILE_CACHE_DIR.glob(\"*_metadata.json\")):\n",
    "        profile_name = metadata_file.stem.replace(\"_metadata\", \"\")\n",
    "        print(f\"     - {profile_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
